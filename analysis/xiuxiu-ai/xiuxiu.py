import json
import time

import pandas as pd
import requests


def scrape_huxiu_success():
    # æ¥å£åœ°å€
    url = "https://api-data-mini.huxiu.com/hxgpt/agent/ai-product-daily/v3/detail-list"

    # è¯·æ±‚å¤´
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Origin": "https://xiuxiu.huxiu.com",
        "Referer": "https://xiuxiu.huxiu.com/",
        "Content-Type": "application/x-www-form-urlencoded",
    }

    # å‚æ•°
    payload = {
        "date": "2026-01-26",
        "platform": "www",
        "page_num": "1",
        "page_size": "50",
    }

    print(f"ğŸš€ æ­£åœ¨è¯·æ±‚æ¥å£: {url}")

    try:
        response = requests.post(url, headers=headers, data=payload)

        if response.status_code == 200:
            res_json = response.json()
            data = res_json.get("data", {})

            # ğŸ¯ æ ¸å¿ƒä¿®æ­£ï¼šæ•°æ®åœ¨ 'event_list' é‡Œ
            event_list = data.get("event_list", [])

            if event_list:
                print(f"âœ… æˆåŠŸè·å–æ•°æ®ï¼å‘ç° {len(event_list)} ä¸ªåˆ†ç±»ç»„")

                all_news = []

                # ç¬¬ä¸€å±‚å¾ªç¯ï¼šéå†åˆ†ç±»ï¼ˆå¦‚â€œäº§å“è¿›å±•â€ã€â€œæŠ€æœ¯æ¼”è¿›â€ï¼‰
                for group in event_list:
                    # è·å–åˆ†ç±»åç§°
                    category = group.get("dynamic_group") or group.get(
                        "dynamic_title", "æœªåˆ†ç±»"
                    )

                    # è·å–è¯¥åˆ†ç±»ä¸‹çš„æ–°é—»åˆ—è¡¨
                    news_items = group.get("group_list", [])
                    print(f"   ğŸ“‚ å¤„ç†åˆ†ç±» [{category}]: åŒ…å« {len(news_items)} æ¡æ–°é—»")

                    # ç¬¬äºŒå±‚å¾ªç¯ï¼šéå†å…·ä½“æ–°é—»
                    for item in news_items:
                        # å¤„ç†äº§å“åï¼ˆæœ‰æ—¶æ˜¯åˆ—è¡¨ ["å…ƒå®", "å…ƒå®æ´¾"]ï¼Œæœ‰æ—¶æ˜¯å­—ç¬¦ä¸²ï¼‰
                        product_name = item.get("product_name", "")
                        if isinstance(product_name, list):
                            product_name = ", ".join(product_name)

                        row = {
                            "åˆ†ç±»": category,
                            "æ ‡é¢˜": item.get("title", ""),
                            "æ‘˜è¦/ç‚¹è¯„": item.get("ai_comment", ""),
                            "æ¶‰åŠäº§å“": product_name,
                            "è¡Œä¸š": item.get("industry", ""),
                            "å‘å¸ƒæ—¶é—´": item.get("publish_datetime", ""),
                        }
                        all_news.append(row)

                # å¯¼å‡º
                if all_news:
                    df = pd.DataFrame(all_news)
                    filename = f"è™å—…AIæ—¥æŠ¥_{time.strftime('%Y%m%d_%H%M')}.csv"
                    df.to_csv(filename, index=False, encoding="utf-8-sig")
                    print("-" * 30)
                    print(f"ğŸ‰ å®Œç¾ï¼å…±æŠ“å– {len(all_news)} æ¡æ•°æ®")
                    print(df[["åˆ†ç±»", "æ ‡é¢˜"]].head(3))
                    print(f"\nğŸ“‚ æ–‡ä»¶å·²ä¿å­˜: {filename}")
                else:
                    print("âš ï¸ event_list ä¸ä¸ºç©ºï¼Œä½†æ²¡æå–åˆ°å…·ä½“æ–°é—»ã€‚")
            else:
                print("âš ï¸ è¿”å›æ•°æ®ä¸­ 'event_list' ä¸ºç©ºã€‚")
                print("æ•°æ®é¢„è§ˆ:", json.dumps(data, ensure_ascii=False)[:300])
        else:
            print("âŒ è¯·æ±‚å¤±è´¥:", response.status_code)

    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    scrape_huxiu_success()
